# 1. 학습 날짜

* 2020-12-10(목)

# 2. 학습시간

* 17:00 ~ 22:00(이노베이션 아카데임 클러스터)

# 3. 학습 범위 및 주제

* 머신러닝에 대해서 공부해보자. 

 # 4. 동료 학습 방법

해당 사항 없음.

# 5. 학습 목표
lec 8-2 공부하기.  
# 6. 상세 학습 내용
복습시간. 
지난시간 공부를 계속하기전에, cifar이라는 회사에 대해서 잠깐 구경하고 가자. 

캐나다의 기부단체중 하나인 cifar이라는 회사에서 지금당장 돈이 안되도 좋은 연구라면 계속해서 연구를 진행하도록 지원해주는 단체. 

lecun교수님을 지원해줌. 

hinton and bengio교수님들은 이 단체에서 지원을 받은 교수님들중 하나였는데, 

처음에 초기 값을 주고 학습을 시켰는데, 이 초기값만 잘 주면, 해결할수 있다는 걸 증명했고, 그리고 큰 문제를 작은 문제로 쪼개어 뉴런처럼 묶어버리면 해결가능하다는걸 논문으로 증명하게 됨으로서 더욱 증명을 받음. 

2010년도까지 이미지 classification을 통해 이미가 무슨 이미지 인지 기계가 판단하게 하면 오류가 28.2%였는데, 2012년 이 오류가 15.3%까지 줄면서 컴퓨터가 이미지를 판단하는 기술에 대해 사람들이 많은 관심을 주기 시작한다.

그리고 2015년의 경우 오차가 3%까지로 줄어들었다. 

이제 컴퓨터는 그림을 설명할수 있게 되었다. 

Deep api learning, 코드를 다른 코드로 변경해주는 것,

딥러닝을 통해 게임도 플레이하게 할수 있고, 알파고를 만들어 바둑도 둘수 있게 만들엇다 .

그렇다면 지금 많은 ai의 기술들이 나오고 있는데, 

그전에는 왜 잘 안되었을까?

우리 주변에는 ai를 통한 기술들이 많이 사용되고 잇다. 

youtube의 자동자막 생성, 구글 검색을 하면, 검색자가 좋아할 만한 데이터를 가장 상단에 보이게 해주고, 가게를 차리더라도 사람들이 어떤물건을 잘 사는지 분석을 해주어서 진열 배치도도 추천받을수 있게 만들엇다. 

교수님은 연구자나 학생이라면, 지금 세계전인 전문가가 되기에 늦지 않았다. 

그리고 수학적으로 복잡하지 않고, 실용적으로 접근을 할수 있다. 지금 딥러닝은 실질적으로 사용할 만큼 정확도가 올랐고, 파이썬이라는 언어로 다룰 수 있어 쉽게 접근할수 있고, 재미또한 있다.

lab 8 tensor manipulation
텐서 쓰는 방법을 알아보자. 

c4_1 이미지
교수님은 이 김밥을 보며 1차원의 배열을 떠올리신다고 한다. 정말 어마무시한 분이다. 
배열은 rank와 shape로 나눌수 있는데, 
위의 배열은 랭크가 1 쉐잎이 7이다. 
int t[7] 이라는 배열에서, 
t[2:5]라고  하면 2,3,4의 배열 선택을 의미한다.  t[4:-1]이면 4,5 번째의 값을 이야기함


shape, rank, axis
{
[1,2]
[3,4]
[5,6]
}
위의 배열은 [3,2]
앞에가 rank 뒤에가 shap의미

[
    [
        [
            [1,2,3,4],
            [5,6,7,8],
            [9,10,11,12]
        ],
        [
            [13,14,15,16],
            [17,18,19,20],
            [21,22,23,24]
        ]
    ]
]
위의 배열은 [1,2,3,4]
[3,3]
[2,2]
위의 두 배열을 더하면 [5,5]가 나온다. 

위의 배열을 그냥 더하면 상관없지만, 문제는 브로드캐스팅에서는 shape가 맞지 않아도 더해지는 경우가 생기는데, 
* 브로드캐스팅
  형태가 다른 모양의 배열을 더해주는 기능. 
  [1,2]
  [3]
  위처럼 shape가 달라도 덧셈이 아능하다.
  [[1., 2.]]
  [3.,4.]
  위처럼 랭크가 달라도 계산이 가능하고
  [[1., 2.]]
  [[3.], [4.]]
  텐스가 다른 경우에도 연산이 가능하다. 

x = [[1., 2.]
     [3., 4.]]

    tf.reduce_mean(x, axis=0).eval()
    이면 array [2., 3.]이 나옴. (1+3)/2, (2+4)/2

    tf.reduce_mean(x, axis=1).eval()
    이면 array [1.5, 3.5]이 나모. (1 +2)/2, (3+4)/2
    tf.reduce_mean(x, axis=-1).eval()
    이면 array [1.5, 3.5]이 나모. (1 +2)/2, (3+4)/2 
# 바로 위에 axis -1 다시보기.     -----------------------------

c4_2
|    ------> axis == 1, -1
| x = [[1.,2.]
↓      [3., 4.]]
axis == 0

reduce sum 합을 구하는 함수
argmax 축의 가장큰 위치를 구하는 함수.
reshape 
[[[0,1,2],
  [3,4,5]],
  
  [[6,7,8],
   [9,10,11]]]
위의 배열의 배열은 [2,2,3] 위의 배열을 아래 배열로 바꿀떄 사용

array [[ 0,1,2],
       [3,4,5],
       [6,7,8],
       [9,10,11]]
tf.reshape(t, shape=[-1,3]).eval()
3은 맨 안의 배열 shape모양이고, -1읒ㄴ 알아서 다 해달라는말. 
reshape(squeeze, expand)
squeeze :
[[0],[1],[2]] 이 배열을 [0,1,2] 바꿈
expand:
tf.expadnd_dims([0,1,2],1).eval() 이렇게 쓰면
[[0],[1],[2]] 형태로 바뀜

one hot 
tf.one_hot[[0],[1],[2],[0]], depth = 3).eval()
[[[1,0,0]],
[[0,1,0]],
[[0,0,1]],
[[1,0,0]]] 이렇게 바꿔줌 

casting:
double을 인트로 바꿈수 있고, true, false를 0또는 1로 바꿀수 있음. 
stack:
배열을 쌓아준다. 

ones and zeros like:
어떤 shape을 원하는 하나의 숫자로 통일해서 형태가 같은 배열을 만듬. 

lecture 9-1
neural nets for Xor
뉴런네트웍을 가지고 xor을 풀어보자!

c4_3 이미지
위의 하나의 logistic으로는 xor을 못푼다느게 증명이 되었다. 

c4_4 이미지
위의 사진처럼 여러개의 logistic으로 접근하게 된다면 xor문제를 해결할수 있다. 
c4_5 이미지
그런데 이렇게 여러개의 logistic이 들어있는 것들을 어떻게 학습을 시킬수 있는가가 문제엿다. 

|x1|x2|xor|
|------|---|---|
|0|0|0(-)|
|0|1|1(+)|
|1|0|1(+)|
|1|1|0(-)|

c4_6 이미지 
위사진에서 오른쪽 밑의 테이블이 형성이 되는데 저렇게 나오는지 살펴보자. 
H(X) = XW + b 이 linear계산식을 사용한다 .

c4_7 이미지
위의 이미지 처럼 H(X)공식을 사용해서 각각의 Y1,Y2값을 구하고 거기에 sigmoid를 사용하여 1,0의 값으로 바꾸어준다. 

이런형태를 통해 NH를 구현햇다. (네트워크)
 c4_8
 > 어떤 원리냐면, 가설에, x와 w 그리고 b값을 넣고 계산을 했을때 만약 0모다 크면 가상의 w 직선보다 위에 위치하게 되고 그 값들을 sigmoid를 진행하여 참의 1로 인식하는것이다.

 lec 9-x 미분의 기본지식 
 c4_9 이미지

f(x)에 대해 정의 해주셨고, 미분을 진행하는 방법을 알려주셨다. 

lec 9-2 딥넷트웤 학습시키기. 
지난 시간에 xor을 두개의 유닛으로 구성하면 이것을 풀수 있다는것을 배웟고 이걸 어떻게 학습할수 있는지 배워보자. 

지난시간에 여러게의 logistic이면 xor을 문제를 풀수 ㅇ있었는데, 문제는 이를 학습시키기 위해선, linear떄와 마찬가지로 gradient decent처럼 cost값을 제곡한 값의 합을 구하고 거기에 미분한 값을 구할수 있어야. cost가 0일떄를 구할수 있었다. 

c4_10 이미지
이걸 어떻게 해결했냐면, 마지막에 나온 error값에서 하나씩 뒤로 돌아가면서 학습하는 방법으로 해결햇다. 

이를 back propagation (chain rule) 이라고 한다 .

c4_11 이미지
결국 하나씩 뒤돌아가면서 W가 f에 미치는 영향 x가 f에 미치는 영향, b가 f 에 미치는 영향을 알아내어야한다.  그값은 w에 대해 미분한값 x에 대해미분한값 b에 대해 미분한 값이 될거다. 

* 편미분과 미분의 차이알아보기
> 편미분은 하나의 특정한 값에 대해서 만 미분하는것. 

forward라는 값을 가지고 와서 대입하고, f로 값이 출력되게 된다. 
그리고 기본