# 1. 학습 날짜

* 2020-12-08(월)

# 2. 학습시간

* 17:00 ~ 22:00(이노베이션 아카데임 클러스터)

# 3. 학습 범위 및 주제

* 머신러닝에 대해서 공부해보자. 

 # 4. 동료 학습 방법

해당 사항 없음.

# 5. 학습 목표
로지스틱 클래니피케이션 공부하기. 
logistic classification
정확도가 높은 알고리즘. 

# 6. 상세 학습 내용
복습시간. 

hypothesis H(X) = WX
우리가 가설을 한값과 실제 값을 비교함. 

cost : cost = 1/m 시그마(XY - y)^2
gradient decent 
W := W -미분(W) cost

여기서 cost는 실제 값과 입력된 값의 차이인데, 이 차이의평균을 미분하게 되면 기울기 값이 나오게 되고 이 값을 현재 W의 값에 뺴게 되면 기울기가 0인 W에 도달하게 된다. 

이 때 도달하게 되는 W가 1차원 직선에서 실질적으로 사용하게 되는 기울기의 값이다.  

다시 cost를 정확하게 정리를 하면, 
실제 일차원 직선과, 임의로 입력된 가상 직선의 차이를 수치화한값의 크기를 cost라고 하고 이 값을 W에 대해 미분하게 되면 기울기가 나오게 된다. 

> 변위를 시간에 대해 미분하면, 속도가 되고, 속도를 시간에 대해 미분하면 가속도가 된다. 
반대로 가속도를 적분하게 되면 속도가 되고 속도를 적분하면 변위가 된다 .

모두를 위한 딥러닝강좌 lec5

classfication 실생활 적용 예제
- spam detection : spam or ham
- facebook feed : show or hide
- credit card fraudulent transaction detetion : legitimate (0) or fraud(1)
  신용카드 사용내역이 평상시랑 다르면 이건 잘못된 사용이라고 판단해줌.

  그외에도 주식 시장에서의 거래,
  
  좀더 상세한 예제.
  몇시간을 공부했을떄 합격과 불합격으로나뉘어지나. 

  c2이미지 0
  이 경우 위의 사진처럼 linear regression으로 판단가능해 보이지만, 문제는 
  y좌표가 0 or 1의 값으로 이루어져잇기 때문에. 기울기가 달라져, 합격인데도 불합격인 경우가 생기게 된다. 아래 사진으로 확인. 
  c2_1 이미지

  또다른 문제는 logistic Hypothesis 의 경우, H(x) = Wx + b 인데, 0~1이라는 범위의 값만 구하게 된다면, 

  W가 0.1일떄 x 가 1~11t시간 정도라, h(x) < 0~1.정도의 값으로 나오겠지만, 

  만약 x가 500시간 정도 공부한 사람이 나오게 되어 버린다면, h가 50이 되면서 보기가 안좋아진다 그래서 이것을 해결하기 위해서 0~1정도의 출력값이 나오는 함수를 새로 정의 하기로 햇다. 

c2_2 이미지 
그래서 위와같이 1또는 0으로 수렴하는 함수를 만들엇고, 

그 함수가 바로 logistic Hypothesis 
c2_3 이미지

5-2 
cost function 과 cost funtion 을 최소화하는 gradient decent 를 알아본다. 
 

이번에 코스트를 구하는데는 문제가 하나 있다. 
calassfication으로 hypothesis를 바꾼거니까, 거기 에 맞는 cost도 바꾸어주어야한다. 

그래서 우리는 이를 해결하기 위해 

기존의 U자형 모양으로 나오는,  gradient기울기 타기에서 이번에느 새로운 모양의 cost funtion을 구해보았다. 

아래 그림과 같다. 
c2_4

왼쪽은 y가 1일때 의 모습인데, 
실제값이 y = 1 일때
예측값이 y = 1 로 일치한다면,
cost(1)는 0으로 정상적으로 나오고,
반대로 예측값 y = 0 이나온다면, 
cost는 무한대로 발산하고, 틀렸다는걸 정확하게 표시하게 해준다. 

결론 진짜 결과 값을 보고 다른 함수를 넣어주어야하는데, 

이렇게 코드를 짜주면 if 문이 들어가면서 코드가 봅잡해 지게 된다 

이걸 해결할려고 또 새로운 함수를 만들었단다. 
지독하다 개발자 및 수학자들.. 

c2_5이미지 

자세히 보면 간단하다. 
y가 0일떄는 앞의 식은 0으로 바껴 없어지고 1일때는 앞에 식만 남고 뒤에식은 없어진다. 


> 지금 이해가안되는건, 처음 사용한 linear 함수의 경우 U자 모양의 cost-W그래프가 나왔는데, 
classfication의경우 U자는 U자이지만 꼬불꼬불한 모양의 함수 모양이 나오게 되엇다. 이렇게 차이가 나게된 이유를 알아보자. 

> 지수함수 e 의 특성을 좀더 자세히 살펴보고 로그도 오래만에 보니까 다시한번 살펴보자. 

이제 tensolflow를 써서 직접함수를 써보자. 

c2_6 이미지. 

위의 사진처럼 메트릭스를 사용하여, 진행을 했다. 


W는 입력값이 2개라서 [2,1]이고 뒤에 1은 나가는 값이 1이라서 1로 해주었다. 
b의 경우, 항상 나가는 값의 개수와같다. 

> 여기서 사용되는 hypothesis와 log(hypothesis)의 그래프를 그려보자.


lec 6-1
여러개의 클래스가 있을떄 그것을 예측하는 멀티노미어 클래스피캐이션 그중에서도 가장 많이 사용되는 softmax classification에 대해서 알아보겟습니다. 

복습
H(x), G(z)
h(x)로 기울기를 통해 임의의 값을 추측, G(z)를 통해 0또는 1로 결과값이 나오는 값을 추측

multinomial classification
c2_7 이미지
기존의 매트릭스의 1차 linear에서 sigmoid로 출력하기 위한 방법을 이번 장에서 공부하겟다. (0~1)

c2_8 이미지.
위의 형태 처럼 전체 sum을 1로 바꾸어 주고, 모든 값들이 0~1사이 값이 나오도록 만들어준다. 

이렇게 나온 값들을 확률로 생각해서 사용할수도 잇다. 

이제 이 예측값과 실제 값을 비교해서 트루인지 거짓인지 구분을 한다 .

지금 우리가 사용할 cross-entropy cost function 은 
c2_9 이미지 밑의 사진 처럼 생겻고,

위의 식을 예제로 접해 보면
c2_10

위의 식처럼 나오게 된다. 여기서 중요한건 로그 앞에 마이너스가 있다는것. 

행력의 곱에서 코스트가 무한대로 늘어나는 순간, 코스트는 커지므로 일치하지 않게 된다. 

이제 실제값과 예상값의 차이를 구하는 방법을 이해하엿고 다음으로는 그 값들의 평균을 내고 W에 다가 기울기를 뺴서 cost가 0 인 값을 구하면 된다. 


lec6-2
one-hot과 reshape 한번 살펴 보기. 
score을 구하는 방법은 기존의 방법.(다시 살표보기)
score 어떤 값들을 가지고 sofrtmax을 통과시기면 확률로 나오게 된다. 




