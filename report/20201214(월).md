# 1. 학습 날짜

* 2020-12-11(금)

# 2. 학습시간

* 17:00 ~ 22:00(이노베이션 아카데임 클러스터)

# 3. 학습 범위 및 주제

* 머신러닝 용어 정리.

 # 4. 동료 학습 방법

해당 사항 없음.

# 5. 학습 목표
전체 용어정리

# 6. 상세 학습 내용
Linear Regression (선형 회귀)

> 회귀란 특정한 value를 지도학습을 통해 예측하는 것을 말합니다.

# 가설(모델) 제작 : 1차원라고 가설을 세움
hypothesis = x_train * W + b

> cost = tf.reduce_mean(tf.square(hypothesis - y_train))

오차를 제곱해서 평균을 구하면 2차원의 함수가 제작된다. 
제곱하는 이유는 절대값을 사용할 수도 있지만 음수를 방지하기 위해 사용하는 것 같다.
reduce_mean() : 평균 구하는 메소드
square() : 제곱을 구하는 메소드


- 표준화 정규화 차이점 다시보기.

- 기계 학습 알고리즘은 지도 학습(supervised learning)과 비지도 학습(unsupervised learning)


 Mean Square Error(MSE) 용어
 ![텍스트](https://wikimedia.org/api/rest_v1/media/math/render/svg/67b9ac7353c6a2710e35180238efe54faf4d9c15https://wikimedia.org/api/rest_v1/media/math/render/svg/67b9ac7353c6a2710e35180238efe54faf4d9c15)


 Multiple Features 

 기존에는 하나의 X로 Y를 예측하는 상황이였습니다. 하지만 실제 현실에서는 다양한 변수들 즉, 여러 개의 X를 통하여 Y를 예측하는 경우의 문제가 대부분입니다. 그러므로 우리는 4개의 Features를 가지는 문제에 대해서 알아보도록 하겠습니다. 아래의 그림에서는 Size, Number of bedrooms, Number of floors, Age of home이라는 4가지 변수를 통해서 Price를 예측하고 있습니다. 앞으로 이러한 다중 변수 상황에 대비하기위해 표기법을 정의하고 넘어가도록 하겠습니다.

 이렇게 입력값이 여러개일떈, gradient desent 구하는것도 다르게 해주어야한다. 

 Feature Scaling 

 Feature Scaling에 대한 아이디어는 Multiple Features를 갖는 문제에서 각각의 Feature들이 비슷한 Scale 즉, 비슷하거나 같은 범위를 갖는다면 Gradient Discent가 더욱 빨리 Optima로 수렴가능하다는 생각에서 시작되었습니다. 아래의 그림에서 왼쪽과 같이 2개의 Feautre가 갖는 범위의 차이가 많이 나는 경우에는 원에서 많이 찌그러진 타원의 모양을 갖게 될 것이고 Global Optima를 구하는데 오랜 시간이 소모될 것입니다.

 하지만 오른쪽 그림과 같이 각 Feature의 값을 최대값 or 최대값-최소값 등과 같은 값으로 나누어준 값으로 치환하여 계산한다면 타원보다 원에 가까운 모양이 될 것이고 더 적은 Gradient Discent 연산을 통해 Global Optima에 도달하게 될 것입니다. 
  ![텍스트](https://t1.daumcdn.net/cfile/tistory/992B3D4E5A48D7F836)


 Automatic Convergence Test

  Automatic Convergence Test는 수렴여부를 자동으로 판단해주는 알고리즘으로 수렴했는지를 판단하는 기준은 J(θ) 의 감소량이 어떤 작은 값 E보다 작을 때를 기준으로 수렴했다고 판단합니다. 일반적으로 E는 10−3 이지만 문제에 따라서 달라지므로 E를 정하는 것 조차도 또다른 문제가 된다. 그러므로 이 알고리즘을 활용하는 것 보다는 그래프를 그려 확인하는 편이 좋습니다.

  다항회귀


  logistic 기존의 linear로는 특정값 이하 또는 이상일떄 결과 값이 0또는 1로 바꾸는 작업을하면, 오차가 생긴다. 이를 해결하기 위해 logistic을 사용한다. 
   Classification,Binary 
# 스터디 

   미분하는거 정리하기
![텍스트](http://www.ktword.co.kr/img_data/4933_1.JPG)
미분 사이트
https://www.derivative-calculator.net/

- Regression: 숫자를 예측

- Classification: 정해진 카테고리를 정하는 것: Pass(1) 또는 Fail(0) 으로 판단한다.

g(z)의 결과가 0 ~ 1 사이에 수렴되는 것을 sigmoid(시그모이드) 함수라 하고, Logistic hypothesis 이라고도 한다. 

hypothesis 가 바꼈으니까, costfuntion도 바꿔주어야한다. 
![텍스트](https://t1.daumcdn.net/cfile/tistory/99A31C505B3C770D2E)

그런데, y값에 따라 일일이 계산하기 힘드니까, 단순히, ![텍스트](https://t1.daumcdn.net/cfile/tistory/996C213D5B3C77DE0B)
이런형태로 바꾼다. 


로지스틱 가설과, sigmod은 0~1의 값으로 바꿔주는거고, 
softmax는 총 값을 1로 바꾸고 , 확률로 바꾸는거다. 
그리고 그 확률을 0또는 1로 바꾸는걸, one hot 이라고 한다. 

Data (X) preprocessing

1. 정규화 (normalization)
2. 표준화 (standarization)
3. 과적화 
overffiting/

오버피팅이 나왔을떄 고치는 방법,
More training data!
• Reduce the number of features
• Regularization

온라인 러닝. 
한번에 많은 입력대신 100만개가 있을떄 10만개씩 나누어 교육시키는데 10만개의 교육이 더해질떄 기존의 학습도추가됨. 

vanishing gradient
레이어가 길어지면 초기 W의 영향력이 작아지는 이유
sigmoid를 사용해서 이를 해결하기 위해서 relu 사용

RBM Restricted Boatman Machine 
일단 초기 입력값에 아무 w나 넣고 그리고, Backward 방법으로, 출력된 결과 값에 다시 w를 이용해서 입력값을 출력한다. 그 입력값과  새롭게 w로 구한 입력값을 비교하여 둘의 차가 가장 작은 w를 사용한다. 
이를 rbm이라고함.


더 간단하게 나온건 , xavier이란 함수가 나왓다. 

그냥 xavier함수를 사용하면, 가까운 w를 구해준다. 

dropout
앞에서 우리는 통계적인 모델을 정규화(regularize)하는 전통적인 방법을 알아봤습니다. 가중치의 크기 ( ℓ2​  norm)을 패널티로 사용해서, 가중치 값이 강제로 작아지도록 했습니다. 확률적인 용어로 말하자면, 가우시안 프리어(Gaussian prior)를 가중치 값에 적용한다고 할 수 있습니다. 하지만, 더 직관적인 함수의 용어를 사용하면, 가중치 값들이 다른 특성(feature)들 사이에 더 확산되도록 하고, 잠재적으로 가짜 연관들에 더 적게 의존되도록 모델을 학습시킨다고 할 수 있습니다.


reshape 다시보기. 

활성화함수(Activation Function)
는 어떤거냐면요. 

logistic, linear 같은것들을 활성화 함수라고 하네요. 

loss함수는 그냥 cost값 더 한것들. 