# 1. 학습 날짜

* 2020-12-11(금)

# 2. 학습시간

* 17:00 ~ 22:00(이노베이션 아카데임 클러스터)

# 3. 학습 범위 및 주제

* 머신러닝에 대해서 공부해보자. 

 # 4. 동료 학습 방법

해당 사항 없음.

# 5. 학습 목표
lec 10-1  
# 6. 상세 학습 내용

lecture 10-1
NN for XOR


lecture 10-2
Initialize weights in a smart way

초기값을 어떻게 줄것인가.

지난시간에 vanishing gradient 라는 문제가 발생한는데, 

c5_1 왼쪽의 초기 W의 값이 뉴런의 개수가 많아질수록 결과값에 미치는 영향이 적어진다. 

그이유는, sigmoid를 하게 되었을떄 1에 가까워 지거나, 0에 가까워 지게 만든다.

드랍아웃을 사용하면 확률이 올라가는 이유. 
그리고 교육시킬떄 많은 데이터가 학습되다보면, 학습이 잘 안이뤄지는 이유. 

lecture 11-1

Cnn introduction 에 대해 알아보겠다. 
(convolutional neural networks)

c5_2  이미지, 

자동차의 이미지가 있다면 뉴런은 저 이미지 전체를 한번에읽는게 아니라, 조각조각 쪼개서 하나씩 인식하는데, 이를 활용한것을 cnn introduction 이라고 한다.
   
이떄 쪼개어 생각하는걸 filter라고 생각한다. 

ex) 32*32*3 mage가 있는다면, filter는 5*5*3 정도 크기의 filter가 있다고 가정을 한다 .

이 5*5*3에는 여려ㅓ가지의 수가 있는데, 이를 어떻게 한개의 수로 뽑아낼수 있을까요?

여기서 하나의 숫자로 만드는 방법은, Wx+b의 방법으로 만든다. 이렇게 5*5*3의 레이어 에서 하나씩 아웃풋 값이 나온게 된다. 

c5_4

여기서 stride는 5*5*3이 한번 움직일떄 이동하는 단위이고몇개의 5*5*3이 나오는지 경우의 수를 알게된다. 

c5_6 이미지, 5*5*3을 convolution layer로 쌓으면 activation maps는 (28,28,6)으로 쌓을수 잇다. 

c5_5 이미지, 

위의 필터는 콘볼루션 레이어를 더 쌓아서 두껍게 만들수 있다. 


그러면 우리가 weight에 사용할수 있는 베리어블의 개수는 몇개일까.
이 값들을 초기에 랜덤으로 주어지지만 학습을 통해 적당한 값으로 주어지게된다.

11-2 cnn introduction:Max pooling and others 지난시간에 cnn에 대해서 알아보았고 이번시간에는 max pooling 과 others에 대해서 알아보도록 하겟습니다. 

c5_7 이미지
relu다으멩 pool이라는걸 진행을 햇다.

c5_8 이미지. 

위의 사진을 보면 여러개의 레이어가 쌓여잇는데, 그중 한개의 레이어를 뽑아내고 그 레이어를 샘플링을하고 그 값을 다시 쌓는게, pooling 이다. 

c5_9 풀링의 예를 가지고 왓느데, stirde가 2 니까 4개의 레이어가 나오고 각각의 레이어중 하나의 숫자만 골라서 sampling으로 만드는데, 보통은 가장큰 숫자로 sampling한다. 그래서 오른쪽 모습과 같은 2*2sampling이 만들어 진걸 볼수 잇다 .

lecture 11-3 : CNN case study
사람들은 cnn을 어떻게 응용했을까?


lab 11-2 cnn mnist 99%


